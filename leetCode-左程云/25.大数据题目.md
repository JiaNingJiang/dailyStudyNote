## 一、大数据题目解题思路

1. 哈希函数可以把数据按照种类均匀分流
2. 布隆过滤器用于集合的建立与查询，可以节省大量的空间
3. 一致性哈希解决数据服务器的负载管理问题
4. 利用并查集结构做岛屿问题的并行计算
5. 位图解决某一范围上数字的出现情况，并可以节省大量空间
6. 利用分段统计思想、并进一步节省大量空间
7. 利用堆、外排序来做多个处理单元的结果合并



## 二、分段统计题目一

32位无符号整数的范围是 0~4,294,967,295（42亿+），现在有一个正好包含40亿个无符号整数的文件，所以在整个范围中必然存在没有出现过的数字。最多使用1GB内存，怎样找到所有的未出现过的数？

> 思路一：采用哈希函数
>
> 将文件中每一个出现过的整型数计算出哈希值，然后哈希值%100(将哈希值分配到100个区域)。然后我们对每一个区域进行词频统计。这样我们可以对所有出现过的数完成词频统计，也就可以找到未出现过的数。
>
> 这种方法占用的内存空间为：8 * 40亿 /100= 320 亿 字节/100 ≈ 32G/100 = 0.32G

进阶：

假设内存限制为10MB、3KB、设置是有限几个变量。但是只需要找到任意一个没出现过的数即可。

通过修改上述解题方法的区域个数(比100更多的区域)，也是可以完成这个任务的。但是我们要介绍另一种解题方法：

> 思路二：
>
> 假设我们只有3KB的内存空间，他能存储一个的32位无符号整型数组长度为 :  3KB/4B = 3*2^8 = 3 * 256。这里我们取整形数组长度为256 = 2^8(长度必须是2的幂的形式，而2^9超过3KB限制，因此只能是2^8)。arr = [256]int
>
> 因为我们要统计的是32位无符号整数中没出现过的数字，因此我们把32位无符号整数分成2^8个区域，arr数组的每一个元素记录一个区域中数的个数。这样每一个区域的大小就是 2^32/2^8 = 2^24 = 16,777,216。
>
> 接下来挨个读取文件中的整数，将当前数存放到对应的区域中。因为文件只有40亿个数(小于42亿)，因此必然有区域实际拥有的整数个数小于16,777,216（也有可能区域的整数个数大于16,777,216，因为文件中可能存在重复的整数。因此这种方法并不精确，但确实能找到1个不存在的数）
>
> 针对于这一个个数不够的区域，我们再将其均分为 2^8个区域，即 2^24/2^8 = 2^16 = 65536。以此类推，直到我们划分到一个区域负责1个整数的情况，此时我们可以找到1个文件中没有的整数。
>
> 最多需要进行32/8 = 4次迭代。
>
> 
>
> 假设我们只能用有限几个变量，比如只有两个变量。解法思路相同，只不过每次分成2个区域。
>
> 此时，最多需要进行32/1 = 32次迭代。



## 三、题目二

有一个包含100亿个URL的大文件，假设每个URL占用64B，请找出其中所有重复的URL

> 思路一：采用哈希函数进行均匀分流
>
> 将每个URL求哈希，然后哈希%m，根据哈希求模的结果将各个URL均匀分配到m个子文件。最后在各个子文件中查询重复出现的URL。最后在进行汇总
>
> 
>
> 思路二：使用布隆过滤器
>
> 每个URL都使用m个哈希函数计算，将对应结果为1的位涂黑，为0的位涂白。如果两个URL涂黑的位置都相同，意味着这两个URL重复

进阶：某搜索公司一天的用户搜索词汇是海量的（百亿数据量），请设计出一种求出每天热门`Top100`词汇的可行方法。

> 思路：采用哈希函数分流 + 外置堆的办法解决
>
> 将每个用户搜索词汇求哈希，然后哈希%m，根据哈希求模的结果将各个热词均匀分配到m个子文件。
>
> 这样在每个子文件就可以有子文件自己的Top100词汇，每个子文件在统计词频热度时采用大根堆的数据结构，频度最高的词汇排在大根堆的根节点。
>
> 准备一个额外的辅助大根堆。每次从m个大根堆中获取堆顶元素，将其插入到辅助大根堆中，然后让大根堆弹出根节点（m个大根堆中频度最高的词汇），假设弹出的是3号大根堆的根节点，那么弹出后辅助大根堆需要再从3号大根堆再获取堆顶元素插入到自己，然后进行下一次堆顶弹出.....
>
> 

## 四、题目三

32位无符号整数的范围是0~4,294,967,295，现在有40亿个无符号整数，可以使用最多1GB的内存，找出所有出现了两次的数。

> 思路一：使用哈希表分流
>
> 在分流后的小文件中，每条记录包含两个值：数据值本身，数据值出现的次数。这会占用2*4字节 = 8字节，1GB/8B = 2^30 / 2^3 = 2^27 = 134,217,728条，即是说每个小文件最多包含 2^27条记录，整个40亿个数需要分成 40/1.35≈30 个小文件进行处理。
>
> 
>
> 思路二：使用位图
>
> 通常位图中1bit表示一个数，如果该数出现了那么 = '1'，如果没出现那么 = '0'。但是此题中需要统计出现两次的数。因此我们至少需要2bit来表示一个数： 00表示没出现，01表示出现一次，10表示出现两次，11表示出现三次及以上。
>
> 要统计40亿个数，我们至少需要 40亿 * 2 = 80亿 bit 的内存空间。而1GB = 8*2^30 = 8 * 10.7 亿 ≈ 86亿。因此空间是足够的。

## 五、问题四

使用最多`10MB`的内存，在40亿个无符号整数中找到中位数

> 思路：分段统计
>
> 10MB 最多容纳 10MB/4B = 2.5M = 2.5*2^20个无符号整形数字，取最大但不超过 2.5 * 2^20的2的幂次方个数，组成数组，数组arr = [2^20]int。
>
> 因为无符号整数出现的范围是 0~2^32-1(0~4294967295)。我们需要将2^32个数均分为2^20个区域，每个区域的大小就是 2^32/2^20 = 2^12。arr[]数组的每一位代表这个区域中数值的个数。假设此时前方所有区域累加起来，一共有18亿个数据，而下一区域而数据个数又是5亿，那么中位数必然出现在下一区域中。
>
> 接下来，我们使用相同的方法对下一区域进行均分……直到前一个区域的总数据量加上当前数的数据量 = 20亿，那么我们找到了这个中位数。

## 六、问题五

假设在硬盘中有一个大小为10GB的文件，里面存放的都是无序的int整数。要求使用5G的内存空间，将这个10GB的文件里面的整数变成有序的存到硬盘中。

> 思路一：使用小根堆
>
> 5GB的内存，假设我们的小根堆每一个节点的大小为16字节（4B数据本身+4B出现次数+8B其余可能用到的辅助空间），小根堆中每个节点的有效信息是：1.数值本身   2.该数出现的次数。小根堆中节点根据节点存储的数值本身进行排序，数值最小的作为根节点。
>
> 因为一个节点占据16字节，因此5GB内存小根堆的最大规模是：5GB/16B = 5 * 2^30/2^4 = 5 * 2^26。取不超过此结果的2的最大次幂值，为：2^28。即：小根堆节点的个数上限就是2^28。
>
> 10GB的文件中存储的是有符号int，也就是说数值的范围是-2^31~2^31-1。接下来我们将这个范围分区，每个区域大小为2^28，因此可以分为 2^32/2^28 = 2^4 = 16个区域。
>
> 1. 第一个区域是 -2^32 ~ 2^32+2^28-1，我们遍历整个10GB文件，**只将这个范围内的数值放到小根堆中进行排序**。如果出现重复的数只需要将已有节点的出现次数+1即可。遍历完整个文件一次之后，我们将小根堆中的数从顶部挨个弹出，根据出现次数写入到一个文件中。这样我们就完成了对 -2^32 ~ 2^32+2^28-1范围内数据的排序
> 2. 接着我们对其余的区域进行排序，直到所有区域完成排序……
>
> 
>
> 思路二：使用大根堆
>
> 假设可以使用的内存空间只有有限个数变量（假设3个）。
>
> 我们需要创建一个容量为3的大根堆，也就是说我们每次只能完成3类数的排序。
>
> 准备一个额外的辅助变量Y
>
> 1. 第一次从头开始遍历整个文件，将开始的三类数（同一类即同一数值，数值相等的数不占用额外的大根堆节点，只是已有节点的出现次数+1）添加到大根堆中。填满之后，开始有规则的淘汰和更新：如果下一个遍历的数大于大根堆根节点，因为不可能作为最小的三个值，因此直接忽略；如果下一个遍历的数小于大根堆节点，则淘汰掉根节点并将其加入，接着调整大根堆（因为可能比其他几个值也小）；如果下一个遍历的数等于大根堆内节点，则对应节点出现次数+1。当第一次遍历结束，我们获得了文件中最小的三类数，将其写入到目标文件中。同时Y更新为大根堆根节点的数值
> 2. 第二次从头开始遍历整个文件，步骤与第一次相同，除了一点：如果当前遍历值 < Y，则忽略该数即可……
> 3. 第三次、第四次、……、第n次，直到Y大于文件中所有数字。
