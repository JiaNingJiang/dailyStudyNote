## 一、logrus

1. 自定义的日志输出格式，包括日志等级，日志所在行号、所在文件，生成日志的函数（logrus.Log.Sprintf()所在的函数），生成日志的时间等都可以自行添加并调整顺序。
2. 可以选择日志的输出方向，可以同时输出到多个文件（文件包括标准输出设备）
3. 可以编写自定义的 Write 方法，调整日志输出的方式，比如说按分钟切割日志（每过一分钟将创建一个新的日志文件，将新产生的日志输入到其中），或者是按照日志文件大小进行切割
4. 可以编写若干 Hook ，Hook 必须实现两个接口：
   1. func (xxx *XXX) Fire(entry *logrus.Entry) error{}
   2. func (xxx *XXX) Levels() []logrus.Level {}

​	后者是 Hook 针对的日志等级范围，前者是对该范围内的日志做的自定义处理。比如我们可以将特定等级范围的日志单独输出到一个文件中；或者将感兴趣的具有特定前缀的日志消息单独输出到一个文件中。



## 二、下层共识：

1. 以 P2P 形式连接的节点网络，通过节点分区算法将其分解为一个个相互独立的子网络。

2. 在每个子网内部，将节点划分为 Leader 节点和 Follower 节点两大类。

3. 每个节点都有一个交易池，负责存储子网内部产生的交易。每当节点从客户节点收集到交易时，会将其存储在本地的交易池中，并且在子网内部进行广播。

4. 子网内的 Leader 节点周期性的通知子网内全部节点启动一轮共识（发送 pre-prepare Msg，简称PPM），该PPM包含了包含三个重要信息：
   1. 当前共识轮次（round ID），该共识轮次的存在使得子网内支持并发的 PBFT 共识，节点的共识处理器（round Handler）只会接收并处理符合自身 round ID 的共识消息。
   2. 待打包交易顺序（TxOrder []common.Hash），下一区块内包含的交易以及其顺序需要在本分区内完成共识。
5. 子网内的 Follower 节点收到 PPM 消息后，首先需要在本地创建一个新的 round Handler，单独负责此轮的 PBFT 共识。之后，Follower 节点便向子网内其他节点广播一条 Prepare Msg（简称 PM ）。PM 中包含了以下几条重要内容：
   1. round ID
   2. validOrder：当前 Follower 所认为包含在 PPM 中的交易集合中交易的合法情况（若本地有此交易标记为1；若没有标记为0）
6. 在后续的 Prepare 阶段，节点针对收到的 PM 中的 validOrder，确认出多数节点（PBFT 中此数量为 2*f+1 ）认可的交易以及交易顺序。节点将基于此构建下一阶段的消息（Commit Msg，简称CM），将其发送给 Leader 。CM中包含了以下内容：
   1. round ID
   2. Next Block（包含了经过共识的有序交易集合）
   3. 当前节点针对此 CM 消息的数字签名
7. Leader 节点收到各个 Follower 的 CM，将包含相同区块的 CM 聚集在一起，如果消息的数量大于等于 2*f + 1，那么组装出本分区最后要提交的区块，其中包含支持该区块的所有节点的数字签名。
8. 一轮共识结束，销毁本轮使用到的 round Handler。

**对基础 PBFT 算法的改进：**

主要还是体现在算法的TPS的改进，原始的 PBFT 算法，每次针对一个事务（交易）进行一次共识，整体的效率很低，该共识算法主要从两方面进行了改进：

1. 一次针对若干笔交易进行共识，Leader 节点并不是收到一笔交易就立即展开一轮 PBFT 共识，而是会将从客户节点获取的交易存储在本地的交易池中，每个一段时间从中取出部分交易，一起进行共识。
2. 分区内部并行运行多轮 PBFT 共识，Leader 节点每次开启一轮 PBFT 共识，就会创建一个 round Handler，单独负责本轮次的 PBFT 共识，不同轮次的 PBFT 共识之间相互独立。各轮次并行的将分区内获得的交易打包成区块，然后再由分区内的 Leader 节点向区块提交至上层网络，大幅度提高了本分区内的 TPS。能做到这一点的关键还是在于：整个系统是一个双层网络架构，下层分区仅仅负责搜集交易并将其排序构成区块，对于交易的执行和区块的落盘则是由上层分区进行的

### 三、下层共识的视图切换：

1. 启动时机：当分区内的 Follower 节点长期无法收到来自 Leader 的 PPM，也就是说分区内已经很长时间没有进行过 PBFT 共识了，此时 Follower 节点们会认为 Leader 节点已经失效，需要重新选举一个 Leader 节点。

2. 每个 follower 节点向分区内的其他 follower 节点发送一条 view-change Msg（简称 VCM），VCM 内包含了下述信息：

   1. 当前视图 ID , View-ID，这是一个依次递增的变量。

   2. 下一视图ID，在当前视图ID的基础上+1
   

3. 当 follower 节点收到至少 2*f+1 条有效的VCM时，则会认为确实需要更换 Leader 节点，那么它会暂停当前正在运行的 round Handler，同时根据下一视图 ID 计算出新 Leader 是谁，然后在分区内广播 New-View Msg（简称 NVM）。NVM 包括：下一视图ID 和 计算出来的新 Leader 节点，当前正在运行的 roundHandler 的 roundID.
4. 正常情况下，分区内 follower 们计算出来的新 Leader 节点都是同一个，那么当该节点收集到至少 2*f+1 条 NVM 后，就确定自身被选为下一个 Leader 节点，那么它就会在分区内部广播 Leader-Change Msg（简称LCM），通知包括自身在内的其他 Follower 节点更改分区内拓扑结构，将旧 Leader 调整为 Follower，用新的 Leader 去代替。
5. 完成 Leader 更换后，针对于上一轮尚未完成的 round Handler，则继续运行完成共识。
6. 由于本系统是一个双层分区网络，新的分区 Leader 还需要通告其他分区 Leader ，告知他们本分区的 Leader 节点已更新。

**对基础 PBFT View-Change 的改进：**

1. 基础的视图切换面对下面这种情况时可能会浪费一轮时间：当前 Leader 失效，同时应当选为下一 Leader 的节点也失效了。那么当前运行视图切换的 follower 们可能会进行发送多次 VCM 和 NVM ，因为本该回复他们的新 Leader 节点失效了。这就浪费了很多无用的时间和资源。为了解决上述问题，引入了心跳机制：分区内的各个节点之间互相发送心跳包，这样节点就可以事先获知下一轮要选举出来的 Leader 是否还存活，如果已经失活，则直接跳过即可。
2. 针对失效 Leader 节点的恢复，原始视图切换没有提交。对于先前因宕机等原因失效的 Leader 节点，当其重新加入分区网络时，其自身意识不到分区内 Leader 节点已经更换，这就导致其后续无法正常参与分区内的共识。为了解决这一问题，Leader 节点的 View-Change 模块也需要运行，只不过其内容与 Follower 节点不同，仅仅是向分区内的 Follower 节点发送确认请求，确认的内容则是当前分区内 Leader 节点的哈希值，如果发现有超过 2*f+1 的 Follower 节点都认为 Leader 节点已经更换，则当前节点会重构本地的网络拓扑，将自身调整为 Follower ，并更新新的 Leader。

## 四、上层共识

1. 组成：上层分区由各下层分区 Leader 节点，以及提供区块排序服务的特殊 Booter 节点共同组成。
2. Leader 节点的上层模块会缓存来自下层分区提交的区块，每当获取到一个新的下层区块后，当前 Leader 就会在上层分区内对该区块进行广播，这其中就包括了 Booter 节点。
3. Booter 节点从 Leader 节点们广播的区块中，选取一个区块作为整个区块链账本的下一区块，将该区块的 BlockID 放入 CODE_NEXT Msg（简称CNM）中进行广播
4. 其他 Leader 节点在收到该 CNM 后，则会对该区块进行验证（检验是否存在？数字签名数量是否足够？等等），验证通过后对区块内交易进行执行（基于智能合约），然后生成交易执行回执，将所有交易回执求哈希获取区块回执， 最后生成一条 CODE_SIGN Msg（简称 CSM），其中包括：1.区块的验证结果  2. 区块的ID  3.区块回执  4.针对回执的数字签名。将 BOM 发送给 Booter 节点。
5. Booter 节点收到该 CSM ，就可以获取其他 Leader 节点的确认情况，如果超过半数的 Leader 认为此区块合法且执行回执相同，则 Booter 节点会针对此区块再次生成 CODE_CMIT Msg（简称CCM），将其广播给所有 Leader；如果不是，则重新选择一个区块重新发送 CNM，而且会通告所有 Leader 节点对执行的交易进行回滚。
6. Leader 节点们在收到 CCM 后，如果是失败的结果，则会对交易进行回滚，重新等待接收新的 CNM。如果是成功的结果，则会将该区块进行最终的落盘，结束后，Leader 向 Booter 节点发送 CODE_DONE Msg（简称 CDM）。
7. 当 Booter 节点收到多数节点的 CDM 后，选取下一个要落盘的区块，再次进行下一轮区块排序。

**注：为了保证能够实现交易的回滚，在世界状态 Level DB 中每一个 key 需要保存上一个版本的 value。**

## 五、上层共识的鲁棒性保证

上层共识的正常运行，很大程度上依赖于提供排序服务的 Booter 节点，因为 Booter 节点需要提供抗毁性和鲁棒性保证，具体的做法就是提供多个冗余的 Booter 节点，一旦当其 Booter 节点失效，能够快速切换到一个新的正常 Booter，继续为 Leader 节点们提供服务。采用的是 Raft 算法。

1. Booter 节点集群，每一时间只有一个 Booter 节点在与其他 Leader 通信，其他的 Booter 则需要从主 Booter 上同步信息，同步的内容如下：
   1. 上层分区所有 Leader 节点的地址信息
   2. 当前区块链的最新区块哈希 Current Block Hash
   3. 从各个 Leader 处获得的缓存区块
   4. 当前排序服务所处的共识状态，以及所选择的下一个区块
2. Booter 集群内的副本节点需要即使探测主节点的工作状态，一旦发现以下两种情况发生，则进行主节点的更换：
   1. 在 life_cycle（每个节点的该计时器时长不一样，目的就是为了防止有多个节点搜集到了相同票数的 vote，导致需要重新选举 Leader） 结束前，没有收到主节点的心跳包，则认为需要更换主节点
   2. 发现 Current Block Hash 长时间未发生变化，也就说整个区块链系统长时间没有落盘区块。（该情况在进行主节点切换前，需要再**运行一个随机时长的计时器**，随机的原因是确保每个节点启动主节点切换的时机不一样）
3. 完成 Raft 算法中的主节点切换工作：
   1. 最先认为主节点失效的节点，向分区内其它节点发送 Change_Request Msg（CRM），包括新视图ID；同时将自己的 vote--（每个节点在每一个视图下 vote 值为1）
   2. 其它节点对最先收到的 CRM 进行回复，如果发现 New View ID 比自己的 View ID要大，则将自身的 vote 投给发送节点（如果自己的vote还存在）
   3. 如果一个节点率先收集到一半以上的 vote，则会通知其他节点自己将成为新的主节点，广播 New_Leader_Msg(简称 NLM)
   4. 其他副本节点收到该 NLM，则更新本地的网络拓扑，将其更改为新的 Leader。
   5. **注：类似于 PBFT 的视图切换，Booter 分区内的主节点也要在重新恢复连接之后，重新向分区内节点确认自身是否仍是主节点，如果不是则需要作为副本节点运行（从新的主节点处同步状态）。**
4. 新的主节点需要作为新的 Booter 采取以下措施：
   1. 根据之前的信息与 Leader 节点们进行连接，进入到同一个网络分区
   2. 通知 Leader 节点们 Booter 节点已经进行了切换
   3. 根据之前同步到的状态，继续提供区块排序服务。

## 六、区块同步服务

1. 作用：有两种情况会使用到区块同步模块
   1. 情况一：正常情况下，上层分区的 Leader 节点们会执行并落盘区块，但是各分区的 Follower 节点不会进行此操作，因此 Follower 节点必须从所在分区的 Leader 节点处获得最近落盘的区块
   2. 情况二：节点因某些情况从网络中断连了一段时间，重新加入网络时，需要从所在分区的其他节点处同步最新的区块

2. 同步的对象：假设同步者A的区块高度是X，它首先会选择一个同步对象，选择的基准是区块高度差的大小，也就是选择一个具有**最高区块高度**的**合法**节点（如果发现同步过来的区块有非法的，则该节点会被记录到非法名单）进行同步。
3. 同步的过程：
   1. 假设同步者A的区块高度是X，被同步者B的区块高度是Y，要同步的区块范围是 X~Y，同步不会一蹴而就，而是分批进行的。节点 A 一次向节点 B 请求 MaxFetch（默认是20个） 个区块。
   2. 节点 B 收到 节点 A 的同步请求后，首先会判断两个节点主链上的**最近公共祖先区块Z**，然后从 Z 开始向 A 返回 MaxFetch 个区块，如果不到 MaxFetch 个就返回已有所有的。
   3. 节点 A 收到节点 B 返回的区块集合之后，会针对每一个区块进行验证（验证哈希值和包括的数字签名，合法的数字签名需要包含：所在分区PBFT容限数量的投票，上层分区一半节点数量的投票），如果验证通过则落盘到本地；如果验证失败，则后续所有区块都扔掉，同时标记该节点为非法节点，继续选择其他合法节点进行同步。

## 七、区块链存储模块

### 7.1 区块链状态数据的存储

状态数据主要包含以下内容：

1. 当前区块高度、最新的区块哈希
2. 智能合约所涉及的状态量：以简单的转账合约为例，存储了每个账户的账户地址以及该账户的余额。

这部分数据一般以 k-v 的形式出现，在区块链执行交易阶段，需要反复进行更改或写入， 对读写速率要求比较高，因此选择使用 k-v 型数据库 levelDB

### 7.2 区块链区块数据的存储

区块数据就是指存储各种交易的区块账本本身，这些数据本身只是记录交易执行的结果，而且仅在最终区块落盘的时候需要写入，已经落盘的区块也不再会进行修改。

这部分数据写入的频率不是很高，但是可能需要供用户查询。因此，对于这部分数据采用查询功能更加强大的关系型数据库 MySQL 进行存储。

存储的时候，分为两张表：区块表和交易表。

1. 区块表：以区块高度作为主键进行存储
2. 交易表：以 "区块高度 + 区块内交易下标" 的组合作为主键进行存储

有的时候，用户可能对特定分区产生的区块感兴趣，因此在区块表中，针对（区块来源分区字段）建立了索引

有的时候，用户可能对特定合约产生的交易感兴趣，因此在交易表中，针对（合约名称字段，合约函数名称字段）建立了联合索引

## 八、区块链接口模块

第一版基于 Gin 框架实现，主要与区块链前端进行通信

第二版基于 grpc-gateway 实现，同时可以通过 Restful API 和 rpc 完成与区块链后端的通信，前者用于前端与后端的通信，后者用于客户端与后端的通信。
